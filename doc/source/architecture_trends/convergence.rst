架构发展趋势
==================

.. admonition:: 导读

  AI芯片架构的演进展现出一条清晰的轨迹：传统的控制流与数据流两种范式，正在走向融合。控制流架构为了效率，不断引入数据流思想；数据流架构为了通用性，也在增强可编程能力。最终的目标都是构建一个能高效执行主流AI计算，又具备足够灵活性以适应未来算法演进的 **混合式、异构** 的计算平台。

架构设计的共同趋势
--------------------

- **片上内存成为芯片性能重要指标**：所有架构都在不遗余力地 **增大片上SRAM的容量和带宽**。无论是GPU的共享内存/L1/L2缓存、IPU/Cerebras的分布式本地SRAM，还是SambaNova的PMU，其目的都是为了将数据尽可能地留在片上，实现极致的数据复用，这是对抗“内存墙”的有效手段。

- **显式数据移动成为主流**：硬件自动管理的透明缓存机制正在被 **由编译器或软件控制的显式数据搬运** 所取代或补充。NVIDIA Hopper的 **TMA**、IPU的 **BSP模型**、数据流架构天然的 **数据流转**，都体现了这一趋势。显式控制能带来更高的性能可预测性和更优的数据搬运效率。

- **编译器成为核心**：硬件变得越来越异构和专用，手动优化的难度急剧增大。无论是Google的 **XLA**、SambaNova的 **RDU编译器**，还是Graphcore的 **Poplar**，亦或是NVIDIA生态中的 **Triton**，它们的核心任务都是将高层计算图 **自动地** 映射、切分、调度到底层异构硬件上，并规划最优的数据流动路径。

- **异构与专用单元硬件**：通用计算单元已无法满足AI的性能需求。架构普遍采用异构设计，将不同任务交给最高效的硬件处理。这包括：
  
  - **张量计算单元**：GPU的Tensor Core、TPU的脉动阵列、SambaNova的PCU。
  - **控制与标量单元**：可编程的RISC-V核心（如Tenstorrent）、GPU的CUDA Core、TPU的VPU。
  - **数据搬运与处理单元**：NVIDIA的TMA、各种DMA引擎、内存控制器、编解码单元等。

技术路线的相互借鉴
------------------

- **GPU正在“数据流化”**：
  
  - **从算力核心看**：引入 **Tensor Core**，将最核心的矩阵运算交由专用的数据流引擎处理。
  - **从数据移动看**：引入 **TMA**，从隐式的缓存管理走向显式的异步数据块搬运。
  - **从编程模型看**：**CUDA Graph** 允许将一系列Kernel调用预先定义成一个静态图，减少了运行时的调度开销；**Triton** 等语言则让开发者能以更接近数据流（块/Tile级别）的抽象来编写高性能算子。
  - **总结**：GPU的演进就是在其强大的SIMT通用计算基础上，不断“插入”和“暴露”更多的数据流硬件和编程接口。

- **数据流架构正在“通用化”与“生态化”**：
  
  - **从可编程性看**：SambaNova的RDU虽然是为模型生成专用数据通路，但其PCU、PMU本身是可编程的；Tenstorrent等架构更是直接集成了RISC-V通用核心来处理控制流和复杂逻辑。这解决了早期数据流架构灵活性不足的问题。
  - **从软件生态看**：所有新兴的数据流架构都在积极拥抱 **MLIR** 等主流编译器中间表示，以便能更顺畅地接入PyTorch、TensorFlow等生态系统，降低用户的迁移成本。

主流形态
--------------

主流AI芯片架构都是一种 **“分层协作”** 的工作模式：

1.  **高层（应用与框架层）**：开发者使用PyTorch等高级框架定义模型。
2.  **中层（编译器与IR层）**：**MLIR** 等统一中间表示将高层计算图降级，进行与硬件无关的图优化。
3.  **底层（代码生成与硬件映射层）**：Triton、XLA、Poplar等特定于硬件的编译器后端，将计算图块（subgraph）或算子，智能地映射到底层硬件。
4.  **硬件层**：一个由 **通用可编程核心** （如RISC-V或GPU SM）作为“调度员”，调度和控制多个 **专用数据流加速器** （如张量核、脉动阵列）协同工作。数据在这些单元之间，通过 **由编译器静态规划好的、硬件支持的显式数据网络** 高效流动。

这种融合架构既能利用数据流硬件实现主流算子（如GEMM）的极致性能，又能通过通用核心保证处理任意计算的灵活性，最终在性能、能效和通用性之间达到最佳平衡。


