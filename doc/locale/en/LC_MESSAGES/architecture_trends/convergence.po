# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, dahu feng
# This file is distributed under the same license as the npu-sim package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: npu-sim\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-03 14:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: en <LL@li.org>\n"
"Language: en\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/architecture_trends/convergence.rst:2
msgid "架构发展趋势"
msgstr "Architecture Development Trends"

#: ../../source/architecture_trends/convergence.rst:4
msgid "导读"
msgstr "Introduction"

#: ../../source/architecture_trends/convergence.rst:6
msgid ""
"AI芯片架构的演进展现出一条清晰的轨迹：传统的控制流与数据流两种范式，正在走向融合。控制流架构为了效率，不断引入数据流思想；数据流架构为了通用性，也在增强可编程能力。最终的目标都是构建一个能高效执行主流AI计算，又具备足够灵活性以适应未来算法演进的"
" **混合式、异构** 的计算平台。"
msgstr ""
"The evolution of AI chip architecture reveals a clear trajectory: the "
"traditional paradigms of control flow and data flow are converging. Control "
"flow architectures are increasingly incorporating data flow concepts for "
"efficiency, while data flow architectures are enhancing programmability for "
"greater generality. The ultimate goal is to build a **hybrid, "
"heterogeneous** computing platform capable of efficiently executing "
"mainstream AI computations while possessing sufficient flexibility to adapt "
"to future algorithmic advancements."

#: ../../source/architecture_trends/convergence.rst:9
msgid "架构设计的共同趋势"
msgstr "Common Trends in Architecture Design"

#: ../../source/architecture_trends/convergence.rst:11
msgid ""
"**片上内存成为芯片性能重要指标**：所有架构都在不遗余力地 "
"**增大片上SRAM的容量和带宽**。无论是GPU的共享内存/L1/L2缓存、IPU/Cerebras的分布式本地SRAM，还是SambaNova的PMU，其目的都是为了将数据尽可能地留在片上，实现极致的数据复用，这是对抗“内存墙”的有效手段。"
msgstr ""
"On-Chip Memory Becomes a Critical Indicator of Chip Performance: All "
"architectures are making every effort to **increase the capacity and "
"bandwidth of on-chip SRAM**. Whether it is the shared memory/L1/L2 cache of "
"GPUs, the distributed local SRAM of IPUs/Cerebras, or the PMU of SambaNova, "
"the goal is to keep data on-chip as much as possible to achieve ultimate "
"data reuse, which is an effective means to combat the \"memory wall\"."

#: ../../source/architecture_trends/convergence.rst:13
msgid ""
"**显式数据移动成为主流**：硬件自动管理的透明缓存机制正在被 **由编译器或软件控制的显式数据搬运** 所取代或补充。NVIDIA Hopper的 "
"**TMA**、IPU的 **BSP模型**、数据流架构天然的 "
"**数据流转**，都体现了这一趋势。显式控制能带来更高的性能可预测性和更优的数据搬运效率。"
msgstr ""
"**Explicit Data Movement Becomes Mainstream**: Hardware-automated "
"transparent caching mechanisms are being replaced or supplemented by "
"**explicit data movement controlled by compilers or software**. NVIDIA "
"Hopper's **TMA**, IPU's **BSP model**, and the inherent **data streaming** "
"in dataflow architectures all reflect this trend. Explicit control enables "
"higher performance predictability and superior data movement efficiency."

#: ../../source/architecture_trends/convergence.rst:15
msgid ""
"**编译器成为核心**：硬件变得越来越异构和专用，手动优化的难度急剧增大。无论是Google的 **XLA**、SambaNova的 "
"**RDU编译器**，还是Graphcore的 **Poplar**，亦或是NVIDIA生态中的 **Triton**，它们的核心任务都是将高层计算图 "
"**自动地** 映射、切分、调度到底层异构硬件上，并规划最优的数据流动路径。"
msgstr ""
"The Compiler Becomes the Core: Hardware is becoming increasingly "
"heterogeneous and specialized, making manual optimization significantly more"
" difficult. Whether it's Google's **XLA**, SambaNova's **RDU Compiler**, "
"Graphcore's **Poplar**, or NVIDIA's ecosystem tool **Triton**, their core "
"task is to **automatically** map, partition, and schedule high-level "
"computational graphs onto the underlying heterogeneous hardware, while "
"planning the optimal data flow paths."

#: ../../source/architecture_trends/convergence.rst:17
msgid "**异构与专用单元硬件**：通用计算单元已无法满足AI的性能需求。架构普遍采用异构设计，将不同任务交给最高效的硬件处理。这包括："
msgstr ""
"**Heterogeneous and Specialized Unit Hardware**: General-purpose computing "
"units can no longer meet the performance demands of AI. Architectures "
"commonly adopt a heterogeneous design, assigning different tasks to the most"
" efficient hardware. This includes:"

#: ../../source/architecture_trends/convergence.rst:19
msgid "**张量计算单元**：GPU的Tensor Core、TPU的脉动阵列、SambaNova的PCU。"
msgstr ""
"**Tensor Compute Unit**：GPU's Tensor Core, TPU's systolic array, SambaNova's"
" PCU."

#: ../../source/architecture_trends/convergence.rst:20
msgid "**控制与标量单元**：可编程的RISC-V核心（如Tenstorrent）、GPU的CUDA Core、TPU的VPU。"
msgstr ""
"**Control and Scalar Units**: Programmable RISC-V cores (such as "
"Tenstorrent), GPU CUDA Cores, TPU VPUs."

#: ../../source/architecture_trends/convergence.rst:21
msgid "**数据搬运与处理单元**：NVIDIA的TMA、各种DMA引擎、内存控制器、编解码单元等。"
msgstr ""
"**Data Movement and Processing Units**: NVIDIA's TMA, various DMA engines, "
"memory controllers, codec units, etc."

#: ../../source/architecture_trends/convergence.rst:24
msgid "技术路线的相互借鉴"
msgstr "Cross-pollination of technical approaches"

#: ../../source/architecture_trends/convergence.rst:26
msgid "**GPU正在“数据流化”**："
msgstr "**GPU is Becoming \"Data-Streamed\"**："

#: ../../source/architecture_trends/convergence.rst:28
msgid "**从算力核心看**：引入 **Tensor Core**，将最核心的矩阵运算交由专用的数据流引擎处理。"
msgstr ""
"From the perspective of computing core: Introducing **Tensor Core**, which "
"offloads the most critical matrix operations to dedicated dataflow engines."

#: ../../source/architecture_trends/convergence.rst:29
msgid "**从数据移动看**：引入 **TMA**，从隐式的缓存管理走向显式的异步数据块搬运。"
msgstr ""
"**From a data movement perspective:** Introducing **TMA** shifts from "
"implicit cache management to explicit asynchronous data block transfers."

#: ../../source/architecture_trends/convergence.rst:30
msgid ""
"**从编程模型看**：**CUDA Graph** 允许将一系列Kernel调用预先定义成一个静态图，减少了运行时的调度开销；**Triton** "
"等语言则让开发者能以更接近数据流（块/Tile级别）的抽象来编写高性能算子。"
msgstr ""
"**From a programming model perspective**: **CUDA Graph** allows pre-defining"
" a series of kernel calls into a static graph, reducing runtime scheduling "
"overhead; languages like **Triton** enable developers to write high-"
"performance operators using an abstraction closer to the data flow "
"(block/tile level)."

#: ../../source/architecture_trends/convergence.rst:31
msgid "**总结**：GPU的演进就是在其强大的SIMT通用计算基础上，不断“插入”和“暴露”更多的数据流硬件和编程接口。"
msgstr ""
"**Summary**: The evolution of GPUs involves continuously \"inserting\" and "
"\"exposing\" more dataflow hardware and programming interfaces on top of "
"their powerful SIMT general-purpose computing foundation."

#: ../../source/architecture_trends/convergence.rst:33
msgid "**数据流架构正在“通用化”与“生态化”**："
msgstr ""
"Dataflow architecture is becoming \"general-purpose\" and \"ecosystem-"
"oriented\":"

#: ../../source/architecture_trends/convergence.rst:35
msgid ""
"**从可编程性看**：SambaNova的RDU虽然是为模型生成专用数据通路，但其PCU、PMU本身是可编程的；Tenstorrent等架构更是直接集成了RISC-"
"V通用核心来处理控制流和复杂逻辑。这解决了早期数据流架构灵活性不足的问题。"
msgstr ""
"**From a programmability perspective**: While SambaNova's RDU is designed "
"with specialized data paths for model generation, its PCU and PMU are "
"inherently programmable; architectures like Tenstorrent go even further by "
"directly integrating RISC-V general-purpose cores to handle control flow and"
" complex logic. This addresses the flexibility shortcomings of early "
"dataflow architectures."

#: ../../source/architecture_trends/convergence.rst:36
msgid ""
"**从软件生态看**：所有新兴的数据流架构都在积极拥抱 **MLIR** "
"等主流编译器中间表示，以便能更顺畅地接入PyTorch、TensorFlow等生态系统，降低用户的迁移成本。"
msgstr ""
"From the perspective of the software ecosystem: all emerging dataflow "
"architectures are actively embracing mainstream compiler intermediate "
"representations like **MLIR** to more seamlessly integrate with ecosystems "
"such as PyTorch and TensorFlow, thereby reducing user migration costs."

#: ../../source/architecture_trends/convergence.rst:39
msgid "主流形态"
msgstr "Mainstream form"

#: ../../source/architecture_trends/convergence.rst:41
msgid "主流AI芯片架构都是一种 **“分层协作”** 的工作模式："
msgstr ""
"Mainstream AI chip architectures all adopt a **\"layered collaboration\"** "
"working model:"

#: ../../source/architecture_trends/convergence.rst:43
msgid "**高层（应用与框架层）**：开发者使用PyTorch等高级框架定义模型。"
msgstr ""
"**High-Level (Application and Framework Layer)**: Developers use high-level "
"frameworks like PyTorch to define models."

#: ../../source/architecture_trends/convergence.rst:44
msgid "**中层（编译器与IR层）**：**MLIR** 等统一中间表示将高层计算图降级，进行与硬件无关的图优化。"
msgstr ""
"**Middle Layer (Compiler and IR Layer)**: **MLIR** and other unified "
"intermediate representations lower high-level computational graphs, "
"performing hardware-independent graph optimizations."

#: ../../source/architecture_trends/convergence.rst:45
msgid ""
"**底层（代码生成与硬件映射层）**：Triton、XLA、Poplar等特定于硬件的编译器后端，将计算图块（subgraph）或算子，智能地映射到底层硬件。"
msgstr ""
"**Bottom Layer (Code Generation & Hardware Mapping)**: Hardware-specific "
"compiler backends such as Triton, XLA, and Poplar intelligently map "
"computational subgraphs or operators to the underlying hardware."

#: ../../source/architecture_trends/convergence.rst:46
msgid ""
"**硬件层**：一个由 **通用可编程核心** （如RISC-V或GPU SM）作为“调度员”，调度和控制多个 **专用数据流加速器** "
"（如张量核、脉动阵列）协同工作。数据在这些单元之间，通过 **由编译器静态规划好的、硬件支持的显式数据网络** 高效流动。"
msgstr ""
"**Hardware Layer**: A **general-purpose programmable core** (such as RISC-V "
"or GPU SM) acts as a \"dispatcher,\" scheduling and controlling multiple "
"**specialized dataflow accelerators** (such as tensor cores, systolic "
"arrays) to work collaboratively. Data efficiently flows between these units "
"via **an explicitly data network, statically planned by the compiler and "
"hardware-supported**."

#: ../../source/architecture_trends/convergence.rst:48
msgid ""
"这种融合架构既能利用数据流硬件实现主流算子（如GEMM）的极致性能，又能通过通用核心保证处理任意计算的灵活性，最终在性能、能效和通用性之间达到最佳平衡。"
msgstr ""
"This fused architecture leverages dataflow hardware to achieve peak "
"performance for mainstream operators (such as GEMM), while ensuring "
"flexibility for handling arbitrary computations through general-purpose "
"cores, ultimately achieving an optimal balance between performance, energy "
"efficiency, and versatility."
